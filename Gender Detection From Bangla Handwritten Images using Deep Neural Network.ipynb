{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TAb1RyLITwX"
   },
   "source": [
    "#**Problem 2 Male and Female Handwriting Digit**\n",
    " \n",
    "Handwriting is a unique quality of human being, every person has unique handwriting that is very hard to copy but there can be some similarities\n",
    "Between male and female handwriting. we will classify the difference between Male and Female Handwritting Digit using logistic regression.\n",
    " \n",
    "Differences between Male and Female Handwriting Digit can be used in various applications like Archeology, forensic department. In this assignment, our main goal will be to achieve a good result using logistic regression and tuning hyperparameters correctly to get a better result.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "#**Dataset**\n",
    " \n",
    " \n",
    "> For this experiment, we will use the dataset [Ekush](https://shahariarrabby.github.io/ekush/#download) which is available in **Github**. \n",
    "There will be a total of **30830** images, and it was split in a **90:10** ratio. **90%** (**27747**) of data is used in training and **10%** (**3083**) was used in testing.Whole dataset was shuffled before spliting , so that traning and testing dataset can have both label data. \n",
    " \n",
    "**Snapshot from Dataset**\n",
    "\n",
    "><div align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1GhuJfYchOjfNoBtwkpWivPRQinegMYwy\" width=\"600\">\n",
    "</div>\n",
    " \n",
    " \n",
    "#**Experimental Setup**\n",
    " \n",
    " \n",
    "> All of these experiments were performed using Google Colab free GPU, Models were created in PyTorch. \n",
    " \n",
    " \n",
    "During the whole experiment,\n",
    "* The height and width of the input was **28*28 =784** \n",
    "* Output dimension was **(0,1)=2**\n",
    "* Each batch size was different if each setting\n",
    "* The number of iteration was **10000**\n",
    "* **Softmax** activation function was used in output layer\n",
    "* GPU **Tesla T4** was available \n",
    "\n",
    " \n",
    "- **totaldata:** 30830\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "> \n",
    "- **epochs**\n",
    "  - $epochs = iterations \\div \\frac{totaldata}{minibatch}  $\n",
    " \n",
    "We will use different  learning rate to achieve better performance\n",
    " \n",
    "# **Result**\n",
    " \n",
    " \n",
    "| Experiment Number      | Optimizer     | Learning Rate     |  Num of Hidden Layer   | Btach Size |Num of epoch    | |  Accurecy of last 1000 iterations    |\n",
    "| :------------- | :----------: |:----------: | :-----------: | :-----------:  | :-----------: || :-----------: |\n",
    "|  1 |SGD   | 0.02 | 4| 256 | 830   ||50.14  |\n",
    "|  2 |SGD   | 0.01 | 4| 512 |  1660  ||49.17  |\n",
    "|  3 |SGD   | 0.10 | 3| 256 | 830   ||50.66  |\n",
    "|  4 |SGD   | 0.01 | 2| 1024 | 3321   ||49.82  |\n",
    "\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gxns8PIZakG"
   },
   "source": [
    "**Importing All Important Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fKb5x6UbBVY",
    "outputId": "062901e9-50bc-40c6-9482-4d1c0c4c3c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /drive\n",
      "Tesla P4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "from sklearn.utils import shuffle\n",
    "drive.mount('/drive')\n",
    "print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW9huJK_adv9"
   },
   "source": [
    "**Reading CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75q9YEJ2cKiQ"
   },
   "outputs": [],
   "source": [
    "female_df=pd.read_csv(\"/drive/MyDrive/MaleFemaleDatsetCsv/femaleDigits.csv\")\n",
    "male_df=pd.read_csv(\"/drive/MyDrive/MaleFemaleDatsetCsv/maleDigits.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJPo468KBYNP"
   },
   "source": [
    "**setting label 1 for female and 0 for male**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96UJP4GmcjLN"
   },
   "outputs": [],
   "source": [
    "female_df['label']=1\n",
    "male_df['label']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7Xbmr88-76P"
   },
   "source": [
    "**Merging, Normalize and Reshaping  CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22HnvcGnebFV",
    "outputId": "e391a2fd-88e2-48ac-df53-2adfb77ff981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30830, 785)\n",
      "(30830, 784)\n",
      "(30830, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "df_name=[female_df,male_df]\n",
    "dataset_image=pd.concat(df_name,ignore_index=True)\n",
    "dataset_image= shuffle(dataset_image)\n",
    "dataset_label=dataset_image['label']\n",
    "print(dataset_image.shape)\n",
    "dataset_image=dataset_image.drop(labels='label',axis=1)\n",
    "print(dataset_image.shape)\n",
    "dataset_image=dataset_image/255.0\n",
    "dataset_image=dataset_image.values.reshape(-1,28,28,1)\n",
    "print(dataset_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOuuB6W9azKd"
   },
   "source": [
    "**Displaying Image and Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "jeSRUqB6fBt9",
    "outputId": "32d38b76-f4c6-4c62-e426-fd3b518260a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "show_img = dataset_image[0].reshape(28, 28)\n",
    "plt.imshow(show_img, cmap='gray')\n",
    "print(dataset_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDvFaRHc_SVQ"
   },
   "source": [
    "**Train and Test Data Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R02ThzBvfFGd",
    "outputId": "fd3af350-9a50-48a0-a919-4fe2ab47f680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27747\n",
      "27747\n",
      "3083\n",
      "3083\n"
     ]
    }
   ],
   "source": [
    "train_image, test_image,train_label,test_label = train_test_split(dataset_image, dataset_label, test_size=0.1)\n",
    "print(len(train_image))\n",
    "print(len(train_label))\n",
    "print(len(test_image))\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81Dt50rj_bNQ"
   },
   "source": [
    "**Ziping Train, Test Data and Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biE0zczh2bwm"
   },
   "outputs": [],
   "source": [
    "train_dataset = list(zip(test_image,train_label))\n",
    "test_dataset = list(zip(test_image,test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrvmCTaYrNmz",
    "outputId": "b05314d1-9db0-4a7d-edef-b3d7035bf5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Loss: 0.660205602645874. Accuracy: 48.55660071359066\n",
      "Iteration: 2000. Loss: 0.5657833814620972. Accuracy: 49.43237106714239\n",
      "Iteration: 3000. Loss: 0.5582999587059021. Accuracy: 49.918910152448916\n",
      "Iteration: 4000. Loss: 0.41484755277633667. Accuracy: 48.880960103795005\n",
      "Iteration: 5000. Loss: 0.2626202702522278. Accuracy: 50.14596172559195\n",
      "Iteration: 6000. Loss: 0.17708514630794525. Accuracy: 50.21083360363283\n",
      "Iteration: 7000. Loss: 0.08090080320835114. Accuracy: 50.081089847551084\n",
      "Iteration: 8000. Loss: 0.035998135805130005. Accuracy: 50.14596172559195\n",
      "Iteration: 9000. Loss: 0.046478480100631714. Accuracy: 50.14596172559195\n",
      "Iteration: 10000. Loss: 0.07257691025733948. Accuracy: 50.14596172559195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_iters = 10000\n",
    "input_dim = 28*28 # num_features = 784\n",
    "#num_hidden = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False) \n",
    "\n",
    "class DeepNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden):\n",
    "        super().__init__()\n",
    "        ### 1st hidden layer: 784 --> 128\n",
    "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        self.selu_1 = nn.SELU()\n",
    "\n",
    "        ### 2nd hidden layer: 128 --> 128\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        self.selu_2 = nn.SELU()\n",
    "\n",
    "        ### 3rd hidden layer: 128 --> 128\n",
    "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        self.selu_3 = nn.SELU()\n",
    "\n",
    "\n",
    "        ### 4th hidden layer: 128 --> 128\n",
    "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
    "        ### Non-linearity in 4th hidden layer\n",
    "        self.selu_4 = nn.SELU()\n",
    "\n",
    "        ### Output layer: 128 --> 10\n",
    "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### 1st hidden layer\n",
    "        out  = self.linear_1(x)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        out = self.selu_1(out)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        out  = self.linear_2(out)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        out = self.selu_2(out)\n",
    "\n",
    "        ### 3rd hidden layer\n",
    "        out  = self.linear_3(out)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        out = self.selu_3(out)\n",
    "        \n",
    "        ### 4th hidden layer\n",
    "        out  = self.linear_4(out)\n",
    "        ### Non-linearity in 4th hidden layer\n",
    "        out = self.selu_4(out)\n",
    "\n",
    "        # Linear layer (output)\n",
    "        probas  = self.linear_out(out)\n",
    "        return probas\n",
    "\n",
    "# INSTANTIATE MODEL CLASS\n",
    "\n",
    "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
    "                               num_classes = output_dim,\n",
    "                               num_hidden = 128)\n",
    "# To enable GPU\n",
    "model.to(device)\n",
    "\n",
    "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images.float())\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1000 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images.float())\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSFjPjjYrhNB",
    "outputId": "b34a611b-6c95-4d8f-9b05-9c1a6a990d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Loss: 0.6551018357276917. Accuracy: 48.134933506325005\n",
      "Iteration: 2000. Loss: 0.6504107117652893. Accuracy: 51.70288679857282\n",
      "Iteration: 3000. Loss: 0.6485781073570251. Accuracy: 51.475835225429776\n",
      "Iteration: 4000. Loss: 0.6917115449905396. Accuracy: 50.66493674991891\n",
      "Iteration: 5000. Loss: 0.712756335735321. Accuracy: 50.66493674991891\n",
      "Iteration: 6000. Loss: 0.7359727621078491. Accuracy: 49.10801167693805\n",
      "Iteration: 7000. Loss: 0.6371486186981201. Accuracy: 50.66493674991891\n",
      "Iteration: 8000. Loss: 0.6867040991783142. Accuracy: 50.66493674991891\n",
      "Iteration: 9000. Loss: 0.6871746778488159. Accuracy: 49.205319493999355\n",
      "Iteration: 10000. Loss: 1.558924674987793. Accuracy: 49.205319493999355\n",
      "Iteration: 11000. Loss: 0.7020922899246216. Accuracy: 49.17288355497892\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 512\n",
    "num_iters = 10000\n",
    "input_dim = 28*28 # num_features = 784\n",
    "#num_hidden = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False) \n",
    "\n",
    "class DeepNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden):\n",
    "        super().__init__()\n",
    "        ### 1st hidden layer: 784 --> 128\n",
    "        self.linear_1 = nn.Linear(input_size, 128)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "        ### 2nd hidden layer: 128 --> 256\n",
    "        self.linear_2 = nn.Linear(128, 256)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        ### 3rd hidden layer: 256 --> 512\n",
    "        self.linear_3 = nn.Linear(256, 512)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        self.relu_3 = nn.SELU()\n",
    "\n",
    "\n",
    "        ### 4th hidden layer: 512 --> 1024\n",
    "        self.linear_4 = nn.Linear(512, 1024)\n",
    "        ### Non-linearity in 4th hidden layer\n",
    "        self.relu_4 = nn.SELU()\n",
    "\n",
    "        ### Output layer: 100 --> 10\n",
    "        self.linear_out = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### 1st hidden layer\n",
    "        out  = self.linear_1(x)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        out = self.relu_1(out)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        out  = self.linear_2(out)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        out = self.relu_2(out)\n",
    "\n",
    "        ### 3rd hidden layer\n",
    "        out  = self.linear_3(out)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        out = self.relu_3(out)\n",
    "        \n",
    "        ### 4th hidden layer\n",
    "        out  = self.linear_4(out)\n",
    "        ### Non-linearity in 4th hidden layer\n",
    "        out = self.relu_4(out)\n",
    "\n",
    "        # Linear layer (output)\n",
    "        probas  = self.linear_out(out)\n",
    "        return probas\n",
    "\n",
    "# INSTANTIATE MODEL CLASS\n",
    "\n",
    "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
    "                               num_classes = output_dim,\n",
    "                               num_hidden = 128)\n",
    "# To enable GPU\n",
    "model.to(device)\n",
    "\n",
    "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images.float())\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1000 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images.float())\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbOzDr4cvwDy",
    "outputId": "a935569b-5ee0-4c9e-f174-c20ebf71de85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Loss: 1.3946523666381836. Accuracy: 49.237755433019785\n",
      "Iteration: 2000. Loss: 0.7654708027839661. Accuracy: 50.762244566980215\n",
      "Iteration: 3000. Loss: 0.788465678691864. Accuracy: 50.762244566980215\n",
      "Iteration: 4000. Loss: 0.694658637046814. Accuracy: 50.762244566980215\n",
      "Iteration: 5000. Loss: 0.7534566521644592. Accuracy: 49.237755433019785\n",
      "Iteration: 6000. Loss: 0.6943345665931702. Accuracy: 49.237755433019785\n",
      "Iteration: 7000. Loss: 0.7011030912399292. Accuracy: 50.66493674991891\n",
      "Iteration: 8000. Loss: 0.6886188387870789. Accuracy: 50.66493674991891\n",
      "Iteration: 9000. Loss: 0.6943090558052063. Accuracy: 50.66493674991891\n",
      "Iteration: 10000. Loss: 0.6881730556488037. Accuracy: 50.66493674991891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_iters = 10000\n",
    "input_dim = 28*28 # num_features = 784\n",
    "#num_hidden = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.10\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False) \n",
    "\n",
    "class DeepNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden):\n",
    "        super().__init__()\n",
    "        ### 1st hidden layer: 784 --> 128\n",
    "        self.linear_1 = nn.Linear(input_size, 128)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "        ### 2nd hidden layer: 128 --> 256\n",
    "        self.linear_2 = nn.Linear(128, 256)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        ### 3rd hidden layer: 256 --> 512\n",
    "        self.linear_3 = nn.Linear(256, 512)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        self.relu_3 = nn.SELU()\n",
    "\n",
    "\n",
    "        ### Output layer: 512 --> 10\n",
    "        self.linear_out = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### 1st hidden layer\n",
    "        out  = self.linear_1(x)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        out = self.relu_1(out)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        out  = self.linear_2(out)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        out = self.relu_2(out)\n",
    "\n",
    "        ### 3rd hidden layer\n",
    "        out  = self.linear_3(out)\n",
    "        ### Non-linearity in 3rd hidden layer\n",
    "        out = self.relu_3(out)\n",
    "        \n",
    "\n",
    "\n",
    "        # Linear layer (output)\n",
    "        probas  = self.linear_out(out)\n",
    "        return probas\n",
    "\n",
    "# INSTANTIATE MODEL CLASS\n",
    "\n",
    "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
    "                               num_classes = output_dim,\n",
    "                               num_hidden = 128)\n",
    "# To enable GPU\n",
    "model.to(device)\n",
    "\n",
    "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images.float())\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1000 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images.float())\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR6xsuBywumb",
    "outputId": "37ddc2f9-d3ed-48a9-f126-855d4101b94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Loss: 1.342021107673645. Accuracy: 52.87058060330847\n",
      "Iteration: 2000. Loss: 0.03278031945228577. Accuracy: 50.56762893285761\n",
      "Iteration: 3000. Loss: 0.0418388731777668. Accuracy: 50.53519299383717\n",
      "Iteration: 4000. Loss: 0.002530053723603487. Accuracy: 50.14596172559195\n",
      "Iteration: 5000. Loss: 0.07009795308113098. Accuracy: 49.88647421342848\n",
      "Iteration: 6000. Loss: 0.00316959829069674. Accuracy: 49.82160233538761\n",
      "Iteration: 7000. Loss: 0.0015828695613890886. Accuracy: 50.11352578657152\n",
      "Iteration: 8000. Loss: 0.0001606074074516073. Accuracy: 50.11352578657152\n",
      "Iteration: 9000. Loss: 0.0031978576444089413. Accuracy: 49.95134609146935\n",
      "Iteration: 10000. Loss: 0.00012984535715077072. Accuracy: 50.24326954265326\n",
      "Iteration: 11000. Loss: 0.0. Accuracy: 50.081089847551084\n",
      "Iteration: 12000. Loss: 0.07003676146268845. Accuracy: 50.30814142069413\n",
      "Iteration: 13000. Loss: 0.011753994040191174. Accuracy: 49.82160233538761\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1024\n",
    "num_iters = 10000\n",
    "input_dim = 28*28 # num_features = 784\n",
    "#num_hidden = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False) \n",
    "\n",
    "class DeepNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden):\n",
    "        super().__init__()\n",
    "        ### 1st hidden layer: 784 --> 128\n",
    "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        self.selu_1 = nn.LeakyReLU()\n",
    "\n",
    "        ### 2nd hidden layer: 128 --> 128\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        self.selu_2 = nn.SELU()\n",
    "\n",
    "\n",
    "        ### Output layer: 128 --> 10\n",
    "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### 1st hidden layer\n",
    "        out  = self.linear_1(x)\n",
    "        ### Non-linearity in 1st hidden layer\n",
    "        out = self.selu_1(out)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        out  = self.linear_2(out)\n",
    "        ### Non-linearity in 2nd hidden layer\n",
    "        out = self.selu_2(out)\n",
    "\n",
    "\n",
    "        # Linear layer (output)\n",
    "        probas  = self.linear_out(out)\n",
    "        return probas\n",
    "\n",
    "# INSTANTIATE MODEL CLASS\n",
    "\n",
    "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
    "                               num_classes = output_dim,\n",
    "                               num_hidden = 128)\n",
    "# To enable GPU\n",
    "model.to(device)\n",
    "\n",
    "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images.float())\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1000 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images.float())\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKH8wb6yApRC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "160204099_Problem#2_ASS2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
